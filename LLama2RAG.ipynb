{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dbc37b",
   "metadata": {},
   "source": [
    "# RAG system using LLAma2 and Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f4037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we work with  pdf \n",
    "# %pip install pypdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8a734",
   "metadata": {},
   "source": [
    "** REQUIREMENTS **\n",
    "1. transformers \n",
    "2. einops\n",
    "3. accelerate\n",
    "4. langchain\n",
    "5. bitsandbytes - used for quantization ; usually all the models are                           specifically in 16 bits , quantization is converting the\n",
    "                  model from 16 bits to 4 bits ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d1c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the requirements \n",
    "\n",
    "# %pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da51d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **  for EMBEDDING :\n",
    "# %pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0baf3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d43307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198a40d",
   "metadata": {},
   "source": [
    "** ADDITIONAL REQUIREMENTS **\n",
    "1. VectorStoreIndex\n",
    "        -- converts the input string as vectors and maps the index to them  \n",
    "2. SimpleDirectoryReader\n",
    "        -- Reads the pdf file from the directory itself\n",
    "3. ServiceContext \n",
    "        -- helps to combine the LLAma model along with the prompt template \n",
    "4. HuggingFaceLLM\n",
    "        -- Interacts with the hugging face hub and , calls the LLAma2 model\n",
    "5. SimpleInputPrompt\n",
    "        -- for prompting purposes , there are other types of prompts too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc627c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex , SimpleDirectoryReader , ServiceContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1dd225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34dc3c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25426498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts.prompts import SimpleInputPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6584571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36967301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='1fb22dbe-22c5-4031-a383-55c02e5dd90a', embedding=None, metadata={'page_label': '1', 'file_name': '1.IFFCO Oracle Cloud.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\1.IFFCO Oracle Cloud.PDF', 'file_type': 'application/pdf', 'file_size': 1389131, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n1 \\nTechnical Case Study # 6 \\n \\nIFFCO improves IT performance by 7x with \\nOracle Cloud  \\nBy Akshai Parthasarathy,  \\nProduct Marketing Director, Oracle  \\nBy Anil Kumar Gupta,  \\nDirector, IFFCO Ltd  \\n“The board, management, and our stakeholders are very confident that IFFCO has a \\nrobust IT system, one that operates around the clock, pandemic or not. And that system \\nis powered by Oracle.”  \\nA.K. Gupta,  Director of IFFCO  \\n  \\n \\nIndian Farmers Fertiliser Cooperative Ltd. (IFFCO) is the world’s largest manufacturer and \\nmarketeer of fertilizers in the cooperative sector. It serves over 35,000 cooperatives and 55 \\nmillion farmers across India. IFFCO has five manufacturing plants in India and 19 joints \\nventures within India and overseas. It has been using Oracle products for decades and using \\nOracle Cloud Infrastructure (OCI) more recently to modernize its IT estate.  \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eed5e652-0f80-43d7-ad67-22fd35619630', embedding=None, metadata={'page_label': '2', 'file_name': '1.IFFCO Oracle Cloud.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\1.IFFCO Oracle Cloud.PDF', 'file_type': 'application/pdf', 'file_size': 1389131, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n2 \\n \\nFigure 1. Overview of IFFCO  \\nEmpowering farmers with nano urea  \\nIFFCO wants to bring innovative ideas to marke t to improve the livelihood of farmers in \\nIndia. To this end, it recently innovated and launched first of its kind nano urea, portable 500 \\nml (16.9 fl oz) bottled nano urea that replaces 45 kg (99.2 lb) urea bags. Nano urea improves \\ncrop yields by 8 –10%, r educes pesticide and chemical fertilizer use, and mitigates soil \\nerosion. The product also cuts farmers’ spending on urea by half.  \\nNano urea brings new demands to IFFCO’s cloud computing needs. The organization needs a \\nreliable cloud vendor to support the processes of 6 –7 new manufacturing plants during the \\nupcoming year, enhancing its production capability 300 –350 million bottles of nano urea to \\nmeet increasing demand.  \\nBusiness challenges  \\nIFFCO’s cloud adoption is driven by a need to innovate. With cloud i nfrastructure, the \\norganization wanted capabilities for a dynamic business that can adapt to the changing needs \\nof the market while growing fast. IFFCO wanted to shorten innovation cycles, but it was \\ninhibited by rigid on -premises data center deployments. In addition, IT teams were burdened \\nby the overhead of maintaining legacy systems. IFFCO wanted to apply the elasticity and \\navailability of cloud for improving overall performance of applications at lowest possible \\noperational overhead.  \\nSeveral technology systems and platforms in India’s agriculture ecosystem, including \\nIFFCO’s, have become dependent and integrated with each other. For example, IFFCO’s core \\nsystems are linked to key government portals, such as a fertilizer management system, the E -\\nwayBill s ystem for transporting goods, and GST, the nation’s taxation platform. Given the \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ca8007b-7f42-448b-be4d-f9e889d3e499', embedding=None, metadata={'page_label': '3', 'file_name': '1.IFFCO Oracle Cloud.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\1.IFFCO Oracle Cloud.PDF', 'file_type': 'application/pdf', 'file_size': 1389131, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n3 \\nconnected ecosystem, IFFCO needed all its systems, consisting of 100+ applications, to be \\nhighly available. The COVID -19 pandemic further added the ambiguity to scale IT \\nopera tions, but IFFCO’S IT team quickly realized that they could turn this challenge to an \\nopportunity by using the potential of cloud computing to its maximum.  \\nLastly, IFFCO needed to provide the benefits of its technology to all its stakeholders, \\nincluding em ployees, members, transporters, and farmers, some of whom have limited \\nliteracy. To accommodate all stakeholders, IFFCO wanted to add a voice interface to its \\napplications.  \\nSuite of Oracle products used  \\nIFFCO has worked with Oracle for more than two decade s on its critical applications and \\ndatabases that enable business activities. All custom applications are deployed on Windows \\nServer 2012 VMs comprising five WebLogic Servers and nine .NET Servers. In addition, \\nIFFCO used IBM AIX P8 machines for running co rporate, marketing, and ERP applications.  \\nThe following list includes the relevant OCI features, services, and tools that IFFCO utilizes:  \\n\\uf0b7 OCI Compute : OCI provides fast, flexible, and affordable compute c apacity to fit any \\nworkload need from performant bare metal servers and virtual machines (VMs) to \\nlightweight containers.  \\n\\uf0b7 Oracle Exadata Cloud service : Oracle Exadata Cloud service is the best place for \\ncustomers to run Oracle Database workloads in the cloud. Dedicated X8M infrastructure \\nis isolated from other users, allowing database teams to improve security, performance, \\nand uptime for customer databases.  \\n\\uf0b7 Oracle E -Business Suite (EBS) : An Oracle packaged application trusted by thousands of \\norganizations around the world to run their key business operations like enterprise \\nresource planning (ERP)  \\n\\uf0b7 Custom applications on OCI : Homegrown IT or departmental applications that support an \\norganization’s unique business needs  \\n\\uf0b7 Oracle Digital Assistan t: Oracle Digital Assistant is an AI service that offers prebuilt skills \\nand templates to create conversational experiences for your business applications and \\ncustomers through text, chat, and voice interfaces.  \\n\\uf0b7 OCI Load Balancing : OCI Flexible Load Balancing enables customers to distribute web \\nrequests across a fleet of servers or automatically route traffic across fault domains, \\navailability domains, or regions, yielding high availabilit y and fault tolerance for any \\napplication or data source.  \\n\\uf0b7 OCI Web Application Firewall (WAF) : By combining threat intelligence with consistent rule \\nenforcement on an Ora cle flexible load balancer, OCI WAF strengthens defenses and \\nprotects internet -facing application servers and internal applications.  \\n\\uf0b7 OCI FastConnect : OCI FastConnect allows customers to con nect directly to their OCI \\nvirtual cloud network (VCN) through dedicated, private, high -bandwidth connections.  \\nAcross five manufacturing plants, the organization has 100+ applications running on OCI, \\nincluding Oracle E -Business Suite. IFFCO has replaced on -premises infrastructure with OCI, \\nwhich provides better availability, security and scalability.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9b37b454-9ab3-4df0-a3b2-6b746cd44ca9', embedding=None, metadata={'page_label': '4', 'file_name': '1.IFFCO Oracle Cloud.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\1.IFFCO Oracle Cloud.PDF', 'file_type': 'application/pdf', 'file_size': 1389131, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n4 \\n \\n \\nFigure 2. IFFCO’s Infrastructure  \\nMigration path and Oracle solution  \\nIFFCO uses several Oracl e products for its business operations, including Oracle E -Business \\nSuite, eVikas CRM, and several other custom applications for procurement, supply chain \\nmanagement, and HR management. The deployment consists of two OCI regions in India: \\nMumbai and Hydera bad. \\nAs shown in the following figure, IFFCO started on -premises and participated in a proof -of-\\nconcept (PoC) deployment with OCI. After successful validation, the organization migrated \\nall its workloads to the cloud.  \\nThe cloud deployment is in a hub -and-spoke model. The Mumbai region acts as the hub for \\nall other plant infrastructure. The Mumbai region hub is connected to on -premises \\ninfrastructure using FastConnect, which provides low -latency data transfers over a dedicated \\nand secure network path. As sho wn in the following figure, the Mumbai region hosts the \\nproduction environment of legacy applications and E -Business Suite.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10fbeb9c-cb05-4f96-81d2-64dabe25d2ce', embedding=None, metadata={'page_label': '5', 'file_name': '1.IFFCO Oracle Cloud.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\1.IFFCO Oracle Cloud.PDF', 'file_type': 'application/pdf', 'file_size': 1389131, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n5 \\n \\n \\nFigure 3. IFFCO’s hub architecture in the Mumbai region  \\nOCI WAF provides security to traffic following over the internet. All incoming traffic is sent \\nto the public subnet and the  Check Point layer -7 firewall . IFFCO preferred to use separate \\nVCNs for better isolation, more control, and reduced impact radius if a security breach \\noccurs. EBS supports business processes and financial reporting. Exadata is used to host \\ndatabases for EBS and legacy apps.  \\nThe disaster recovery site in the Hyderabad region provides active -active disaster recovery. If \\na failure occurs in the Mumbai region, applications continue to run in the disaster recove ry \\nsite without disruption. This disaster recovery site is used for both legacy applications and E -\\nBusiness Suite and hosts nonproduction environments, developer and Test. The OCI Lift \\nservices team ensured that IFFCO had a smooth migration and deployment.  The team also \\nconducted multiple workshops to enable IFFCO successfully run workloads on OCI but also \\nassisted with planning and designing a flexible and scalable networking architecture based on \\nhub-and-spoke model for production, non -production, and dis aster recovery environments for \\nWindows -based legacy applications, Oracle EBS, and Exadata on OCI.  \\n \\nVideo Link  \\nhttps://youtu.be/WXC2Q7dfVs4   \\n \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a4e8c12-c167-4ab4-bc4d-27a086456bb2', embedding=None, metadata={'page_label': '6', 'file_name': '1.IFFCO Oracle Cloud.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\1.IFFCO Oracle Cloud.PDF', 'file_type': 'application/pdf', 'file_size': 1389131, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n6 \\n \\n \\nFigure 4. IFFCO’s disaster recovery site architecture in the Hyderabad region  \\nThe results  \\nIFFCO’s complete migration to OCI was completed in 57 hours for EBS and 33 hours for \\ncustom applications. The total time to migrate workloads bettered by six hours th an the \\nestimates.  \\nOCI has improved IFFCO’s business agility through better management of IT, faster \\nperformance, and enhanced security. Overall performance has improved 2 –7 times, enabling \\nteams to do more in less time. For example, the business developmen t team can generate \\ntime-bound reports significantly faster than before. Automatic scale -up during month -end and \\nyear-end peak loads simplifies cloud management further.  \\nInnovation roadmap  \\nIFFCO aims to shape the future of the Indian agriculture industry w ith a technology -first \\napproach. As nano urea adoption increases dramatically during the upcoming year, the \\norganization wants to use Oracle’s capabilities to support its new manufacturing plants with \\n100% uptime. As part of the nano urea initiative, it pl ans to use drones and affordable and \\nenergy -efficient technology for farmers to improve crop yields. It’s also looking to expand \\ntwo other digital initiatives: IFFCO BAZAR, an e -commerce and communication portal, and \\nIFFCOYuva, a skill development and empl oyment venture.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7cdaac83-03f0-4591-8aca-a26b5b226f44', embedding=None, metadata={'page_label': '1', 'file_name': '2.Toyota.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\2.Toyota.PDF', 'file_type': 'application/pdf', 'file_size': 971839, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n1 \\n \\nCase Study # 4 \\n \\nToyota moves high -performance workloads \\nto Oracle Cloud  \\nWorld’s largest automaker shifts high -performance computing workloads to Oracle \\nCloud Infrastructure to improve car design and development efficiency.  \\n \\n \\n \\n“We now run our HPC workloads on Oracle Cloud Infrastructure as part of our HPC multicloud \\nstrategy. OCI has incredible performance, and running computational fluid dynamics \\nsimulations with it has allowed us to improve the speed of computations and to opt imize costs. \\nThis is helping us make the development of cars at Toyota more efficient, and produce cars with \\nbetter performance.”  \\nShinichi Noda,  Group Leader, DX Promotion Division, Toyota Motor  \\n \\nProducts list  \\n\\uf0b7 Oracle Cloud Infrastructure  \\n \\nBusiness challenges  \\nTo continue improving the quality of its cars, Toyota is implementing a “Toyota New Global \\nArchitecture.” As a structural innovation that stretches across the company’s global car \\nmanufacturing business, it’s meant to bring dramatic improvements to the basic \\nperformance of Toyota cars. Increasing the efficiency of automobile design and development \\nthrough computational tests and simulations and constantly striving to improve are the keys \\nto making cars tha t have both excellent driving and environmental performance.  \\nToyota had handled high -performance computing workloads in an on -premises \\nenvironment. However, while continuing to use its existing on -premises resources, the \\ncompany began to look into cloud se rvices in order to flexibly meet requests from users, \\nsuch as quickly increasing resources and testing new technologies.  \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8af9d6af-7543-4459-8bf3-50e80f0a9abc', embedding=None, metadata={'page_label': '2', 'file_name': '2.Toyota.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\2.Toyota.PDF', 'file_type': 'application/pdf', 'file_size': 971839, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n2 \\nWhy Toyota Motor chose Oracle  \\nThe company benchmarked a number of cloud service providers as part of its HPC \\nmulticloud strategy, looki ng into performance, cost, computational accuracy, flexibility, \\nstability, and other requirements. After considering these factors, Toyota decided to move \\nthe foundation of its computational simulations to  Oracle  Cloud Infrastructure (OCI)  while \\nalso using existing on -premises systems.  \\n \\nOracle Cloud Infrastructure offers the industry’s first, and only, public cloud with bare \\nmetal HPC computing. It has a low latency of less than 2 microseconds and 100 Gbps of \\nbandwidth, achieved with a Remote Direct Memory Access network. This protocol transfers \\ndata from the memory of a local computer to the memory of a separate, remote computer. \\nOCI’s unique HPC solution allows Toyota to run large and complex computational \\nsimulations, which require a massive amount of computing power, all in the cloud and \\nwithout any compromises in performance.  \\n \\nResults  \\nRunning high -performance workloads for computational simulations on Oracle Cloud \\nInfrastructure has allowed Toyota to increa se the speed and efficiency of car design and \\ndevelopment while also optimizing costs.  \\nAlso, Toyota has dramatically shortened lead time in computing resource procurement, \\nwhich used to take more than six months in the on -premises environment. Now OCI need s \\njust a few days, given there is enough physical space available.  \\nWith OCI, Toyota can now flexibly handle the testing of new technologies, something that \\nwas difficult in the on -premises setup. Toyota achieved a high degree of cost performance \\nwith OCI, as it has allowed the company to shorten the time needed to perform \\ncomputations through improvements to computational ability.  \\n \\nLearn more  \\n\\uf0b7 Toyota , opens in new tab  \\n\\uf0b7 Infographic: OCI for Enterprise (PDF) , opens in new tab  \\n\\uf0b7 Get the guide to Cloud Essentials (PDF) , opens in new tab  \\n\\uf0b7 Understand the Oracle Cloud Infrastructure Platform (PDF) , opens in new tab  \\n\\uf0b7 Get started on Oracle Cloud Infrastructure (PDF) , opens in new tab  \\n\\uf0b7 Learn more about HPC on Oracle Clou d Infrastructure  \\nProducts list  \\n\\uf0b7 Oracle Cloud Infrastructure  \\n  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a67bf40f-c714-4cc0-9f79-54f2c9f92143', embedding=None, metadata={'page_label': '1', 'file_name': '3.Kotak Bank.pdf', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\3.Kotak Bank.pdf', 'file_type': 'application/pdf', 'file_size': 258248, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n1 \\nTechnical Case Study # 14 \\n \\nKotak Mahindra banks on Oracle to boost \\nemployee experience  \\nLeading Indian banking and financial services group reimagines the recruitment process with \\nOracle Fusion Cloud Human Capital Management.  \\n \\n \\n“Over the last year, when people started working from home and they were in lockdown, Oracle \\nWorkforce Health and Safety was used primarily for data collection. That was quite helpful because \\nwe didn't have to re -create the employee database.”  \\nSukhjit Pasricha,  President and G roup Chief Human Resource Officer, Kotak Mahindra Bank  \\nProducts list  \\n• Oracle Cloud HCM  \\nBusiness challenges  \\nKotak Mahindra Bank, headquartered in Mumbai, offers  a range of financial services that \\ncater to retail and corporate customers across urban and rural India. It has a national footprint of \\n1,612 branches and 2,591 ATMs.  \\nOne of the main challenges for the bank, as for the rest of the world, was adapting its \\nbusiness during the pandemic, when most employees were working from home. Using  Oracle Fusion \\nCloud Human Capital Management (HCM) , the bank auto mated its recruitment process, significantly \\nreducing manual intervention.  Oracle Workforce Health and Safety  proved to be helpful in keeping \\nbranch workers all over the country safe.  \\nEstablishing paperless communications was another plus that Oracle Cloud HCM was able to bring. \\nIn a 50,000 -plus employee organization, this shift  made a positive impact on the environment and \\nled to substantial savings for the bank.  \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='09fb0cb0-2e4e-49ff-b20a-2c62786a88f9', embedding=None, metadata={'page_label': '2', 'file_name': '3.Kotak Bank.pdf', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\3.Kotak Bank.pdf', 'file_type': 'application/pdf', 'file_size': 258248, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n2 \\nWhy Kotak Mahindra Bank Chose Oracle  \\nThe primary reason Kotak Mahindra selected Oracle Cloud HCM was due to its ease of use \\non mobile devices. This helped the organizat ion create an internal mobile app and provide a single \\npoint of access for employees, further supporting adoption. Ease of migration from Oracle E -\\nBusiness Suite and on -premises systems to Oracle Cloud HCM was another key driver. Cost, \\nscalability, and fut ure readiness also were factors in Kotak Mahindra’s decision to move forward \\nwith Oracle.  \\nResults  \\nBefore adopting Oracle Cloud HCM, the bank had multiple on -premises HR systems, leading \\nto greater maintenance effort and fragmented employee experiences. The  move to a platform that \\ncould manage a large and diverse employee database as well as different business priorities was a \\nplus.  Oracle Learning  and Oracle Recruiting  within  Oracle Talent Management  helped to automate \\npaperless processes and also to move offline workflows online. The integration of HR processes with \\norganizational processes was another notabl e result.  \\nBy enabling digital signatures, online communications, and paperless workflows, the company \\nestimates a cost savings of about Rs 15 lakh per year by drastically cutting printing and courier costs. \\nDue to the self -service modules, the efficency an d productivity of the organization has increased \\nwhile the user experience has improved.  \\nThanks to Oracle Recruiting, the bank anticipates it will be able to streamline its recruitment \\nefforts as it welcomes new staff into its fold.  \\n \\nPartners  \\nKotak Mahindr a Bank enjoyed a well -managed deployment with  Oracle Consulting , which focused \\non project management and delivery.  \\nProducts list  \\n• Oracle Cloud HCM  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4175d9df-7101-4895-ace2-dd7a2bcbd134', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n#since we work with  pdf \\n# %pip install pypdf\\n\\n\\n# ** REQUIREMENTS **\\n# 1. transformers \\n# 2. einops\\n# 3. accelerate\\n# 4. langchain\\n# 5. bitsandbytes - used for quantization ; usually all the models are                           specifically in 16 bits , quantization is converting the\\n#                   model from 16 bits to 4 bits .\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d469e1b-8957-4e1d-a637-1292e7a9db81', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# Installing the requirements \\n\\n# %pip install -q transformers einops accelerate langchain bitsandbytes\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2827aa75-3bd0-41a9-8d73-8198d81acceb', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# **  for EMBEDDING :\\n# %pip install sentence_transformers\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2aa8b431-c7e6-4eb5-b3c5-e4299d5394cc', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# %pip install --upgrade sentence-transformers\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6c3abe1-ef2e-4c5b-bbf2-90f90f355f11', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# %pip install llama_index\\n\\n\\n# ** ADDITIONAL REQUIREMENTS **\\n# 1. VectorStoreIndex\\n#         -- converts the input string as vectors and maps the index to them  \\n# 2. SimpleDirectoryReader\\n#         -- Reads the pdf file from the directory itself\\n# 3. ServiceContext \\n#         -- helps to combine the LLAma model along with the prompt template \\n# 4. HuggingFaceLLM\\n#         -- Interacts with the hugging face hub and , calls the LLAma2 model\\n# 5. SimpleInputPrompt\\n#         -- for prompting purposes , there are other types of prompts too \\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='949eed03-635f-40e6-8016-314c9909710a', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nfrom llama_index.core import VectorStoreIndex , SimpleDirectoryReader , ServiceContext\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fd77e4d-6214-4b11-bc85-89752d780037', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# %pip install llama-index-llms-huggingface\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4a8bcd4c-ad1e-4bc7-aa66-2e6100585486', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nfrom llama_index.llms.huggingface import HuggingFaceLLM\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a283558f-d8e0-4cb1-941e-95d3ab4c0f07', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nfrom llama_index.core.prompts.prompts import SimpleInputPrompt\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a368adf7-d07d-47ea-9481-0fc281b95faa', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# !pip install nbconvert\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4a0283cd-944b-4024-a2ff-afde896900e2', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\ndocuments = SimpleDirectoryReader(\"C:\\\\\\\\Users\\\\\\\\Mugilan\\\\\\\\LLM\\\\\\\\LLama2RAG\").load_data()\\ndocuments\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='09934031-b664-40a5-b5a5-c6b517976dee', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nsystem_prompt = \"\"\"\\nYou are a Q&A assistant . Your goal is to answer the questions as \\naccurately as possible based on the instructions and context provided.    \\n\"\"\"\\n# default prompt supported by llama2\\nquery_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b2d480a3-a905-4d02-aa77-5c3d818753d6', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# %pip install huggingface_hub\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='516c6086-30f1-491e-8a62-e085a34380e5', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# logging into the huggingface\\nfrom huggingface_hub import login\\ntoken = \"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\"\\nlogin(token,add_to_git_credential=True)\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='835322a4-6411-4080-b002-fb0b436be2d3', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\n# %pip install --upgrade transformers torch\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='275e0a1b-6b7b-4607-b96e-225fd61033c0', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nfrom llama_index.llms.huggingface import HuggingFaceLLM\\nimport torch\\nimport os\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda\")\\nelse:\\n    device = torch.device(\"cpu\")\\nprint(\"Using\", device, \"device\")\\n\\n\\nsystem_prompt = \"\"\"\\nYou are a Q&A assistant . Your goal is to answer the questions as \\naccurately as possible based on the instructions and context provided.    \\n\"\"\"\\n# default prompt supported by llama2\\nquery_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\\n\\nllm = HuggingFaceLLM(\\n    context_window=4096,\\n    max_new_tokens=256,\\n    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\\n    system_prompt=system_prompt,\\n    query_wrapper_prompt=query_wrapper_prompt,\\n    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\\n    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\\n    device_map=\"auto\",\\n    # uncomment this if using CUDA to reduce memory usage\\n    model_kwargs={\"torch_dtype\": torch.float16}\\n)\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cd784285-953a-4dc1-a222-e6715aad3d28', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\\nfrom llama_index.core import ServiceContext\\nfrom llama_index.embeddings.langchain import LangchainEmbedding\\n\\nembed_model=LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b1a28f4-7d85-4344-8457-d1ce1b4d48c3', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"\\n\\n\\n# %pip install -U langchain-community\\nget_ipython().run_line_magic('pip', 'install llama-index-embeddings-langchain')\\n\\n\\n# \\n\\n# \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6afd0e85-33a1-4579-8e59-bac7c0806113', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nservice_context=ServiceContext.from_defaults(\\n    chunk_size=1024,\\n    llm=llm,\\n    embed_model=embed_model\\n)\\nservice_context\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2842e785-161a-43fa-b125-eb8e32d40ce4', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nindex=VectorStoreIndex.from_documents(documents,service_context=service_context)\\nindex\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='200aa010-4473-4b00-a437-b77409649471', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nquery_engine=index.as_query_engine()\\n\\n\\n# In[ ]:\\n\\n\\nresponse=query_engine.query(\"what is the product of toyota ?\")\\n\\n\\n# In[ ]:\\n\\n\\nresponse\\n\\n\\n# ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fbc8342-b4c7-47c4-a261-087b13b7536d', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\n\\nresponse=query_engine.query(\"what is IFFCO?\")\\nresponse\\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\").load_data()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b7bdaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a Q&A assistant . Your goal is to answer the questions as \n",
    "accurately as possible based on the instructions and context provided.    \n",
    "\"\"\"\n",
    "# default prompt supported by llama2\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f67eb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c995e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Mugilan\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# logging into the huggingface\n",
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "login(token,add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08eb17d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73699cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\accelerate\\utils\\modeling.py:1365: UserWarning: Current model requires 4096 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# default prompt supported by llama2\u001b[39;00m\n\u001b[0;32m     16\u001b[0m query_wrapper_prompt \u001b[38;5;241m=\u001b[39m SimpleInputPrompt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|USER|>\u001b[39m\u001b[38;5;132;01m{query_str}\u001b[39;00m\u001b[38;5;124m<|ASSISTANT|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdo_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_wrapper_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_wrapper_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# uncomment this if using CUDA to reduce memory usage\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\llms\\huggingface\\base.py:238\u001b[0m, in \u001b[0;36mHuggingFaceLLM.__init__\u001b[1;34m(self, context_window, max_new_tokens, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, is_chat_model, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    239\u001b[0m     model_name, device_map\u001b[38;5;241m=\u001b[39mdevice_map, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[0;32m    240\u001b[0m )\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# check context_window\u001b[39;00m\n\u001b[0;32m    243\u001b[0m config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\modeling_utils.py:3820\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3818\u001b[0m         device_map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_hooks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_deepspeed_zero3_enabled():\n\u001b[1;32m-> 3820\u001b[0m         dispatch_model(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3823\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mpostprocess_model(model)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\accelerate\\big_modeling.py:490\u001b[0m, in \u001b[0;36mdispatch_model\u001b[1;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[0;32m    488\u001b[0m         model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 490\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    491\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m         )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Convert OrderedDict back to dict for easier usage\u001b[39;00m\n\u001b[0;32m    494\u001b[0m model\u001b[38;5;241m.\u001b[39mhf_device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(device_map)\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "import torch\n",
    "import os\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using\", device, \"device\")\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a Q&A assistant . Your goal is to answer the questions as \n",
    "accurately as possible based on the instructions and context provided.    \n",
    "\"\"\"\n",
    "# default prompt supported by llama2\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    device_map=\"auto\",\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98a8fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "embed_model=LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "744f3664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-embeddings-langchain in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-embeddings-langchain) (0.10.39.post1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.6.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.5.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.30.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.32.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.12.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.0.3)\n",
      "Requirement already satisfied: pydantic>=1.10 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.7.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.5.15)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mugilan\\llm\\llama2rag\\ragenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -U langchain-community\n",
    "%pip install llama-index-embeddings-langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e270f167",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7402c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mugilan\\AppData\\Local\\Temp\\ipykernel_22352\\518452908.py:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context=ServiceContext.from_defaults(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x00000240892ACC10>, num_workers=None), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x00000240892ACC10>, id_func=<function default_id_func at 0x00000240C9E38430>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.core.service_context_elements.llama_logger.LlamaLogger object at 0x00000240D9840A90>, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x00000240892ACC10>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_context=ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "901030fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x240ca837f40>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=VectorStoreIndex.from_documents(documents,service_context=service_context)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c60a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e850687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "response=query_engine.query(\"what is the product of toyota ?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c888eff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=' Oracle Cloud Infrastructure.', source_nodes=[NodeWithScore(node=TextNode(id_='8b887a7e-6bae-4d12-9677-a5c3b85527c5', embedding=None, metadata={'page_label': '2', 'file_name': '2.Toyota.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\2.Toyota.PDF', 'file_type': 'application/pdf', 'file_size': 971839, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8af9d6af-7543-4459-8bf3-50e80f0a9abc', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': '2.Toyota.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\2.Toyota.PDF', 'file_type': 'application/pdf', 'file_size': 971839, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, hash='4e555fc5a770c44014eddc7693b69c0565ab9cb38d8c257cd17dc3e357e416a0')}, text='2 \\nWhy Toyota Motor chose Oracle  \\nThe company benchmarked a number of cloud service providers as part of its HPC \\nmulticloud strategy, looki ng into performance, cost, computational accuracy, flexibility, \\nstability, and other requirements. After considering these factors, Toyota decided to move \\nthe foundation of its computational simulations to  Oracle  Cloud Infrastructure (OCI)  while \\nalso using existing on -premises systems.  \\n \\nOracle Cloud Infrastructure offers the industry’s first, and only, public cloud with bare \\nmetal HPC computing. It has a low latency of less than 2 microseconds and 100 Gbps of \\nbandwidth, achieved with a Remote Direct Memory Access network. This protocol transfers \\ndata from the memory of a local computer to the memory of a separate, remote computer. \\nOCI’s unique HPC solution allows Toyota to run large and complex computational \\nsimulations, which require a massive amount of computing power, all in the cloud and \\nwithout any compromises in performance.  \\n \\nResults  \\nRunning high -performance workloads for computational simulations on Oracle Cloud \\nInfrastructure has allowed Toyota to increa se the speed and efficiency of car design and \\ndevelopment while also optimizing costs.  \\nAlso, Toyota has dramatically shortened lead time in computing resource procurement, \\nwhich used to take more than six months in the on -premises environment. Now OCI need s \\njust a few days, given there is enough physical space available.  \\nWith OCI, Toyota can now flexibly handle the testing of new technologies, something that \\nwas difficult in the on -premises setup. Toyota achieved a high degree of cost performance \\nwith OCI, as it has allowed the company to shorten the time needed to perform \\ncomputations through improvements to computational ability.  \\n \\nLearn more  \\n\\uf0b7 Toyota , opens in new tab  \\n\\uf0b7 Infographic: OCI for Enterprise (PDF) , opens in new tab  \\n\\uf0b7 Get the guide to Cloud Essentials (PDF) , opens in new tab  \\n\\uf0b7 Understand the Oracle Cloud Infrastructure Platform (PDF) , opens in new tab  \\n\\uf0b7 Get started on Oracle Cloud Infrastructure (PDF) , opens in new tab  \\n\\uf0b7 Learn more about HPC on Oracle Clou d Infrastructure  \\nProducts list  \\n\\uf0b7 Oracle Cloud Infrastructure', start_char_idx=2, end_char_idx=2223, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.41478709179787526), NodeWithScore(node=TextNode(id_='df884c45-40af-4825-bf3f-362f196b1f53', embedding=None, metadata={'page_label': '1', 'file_name': '2.Toyota.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\2.Toyota.PDF', 'file_type': 'application/pdf', 'file_size': 971839, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7cdaac83-03f0-4591-8aca-a26b5b226f44', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2.Toyota.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\2.Toyota.PDF', 'file_type': 'application/pdf', 'file_size': 971839, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, hash='8fa1dd3466f406b3d3e0a911e2e57a87fe2a47c49b97c7a6c257a64feb73cb4e')}, text='1 \\n \\nCase Study # 4 \\n \\nToyota moves high -performance workloads \\nto Oracle Cloud  \\nWorld’s largest automaker shifts high -performance computing workloads to Oracle \\nCloud Infrastructure to improve car design and development efficiency.  \\n \\n \\n \\n“We now run our HPC workloads on Oracle Cloud Infrastructure as part of our HPC multicloud \\nstrategy. OCI has incredible performance, and running computational fluid dynamics \\nsimulations with it has allowed us to improve the speed of computations and to opt imize costs. \\nThis is helping us make the development of cars at Toyota more efficient, and produce cars with \\nbetter performance.”  \\nShinichi Noda,  Group Leader, DX Promotion Division, Toyota Motor  \\n \\nProducts list  \\n\\uf0b7 Oracle Cloud Infrastructure  \\n \\nBusiness challenges  \\nTo continue improving the quality of its cars, Toyota is implementing a “Toyota New Global \\nArchitecture.” As a structural innovation that stretches across the company’s global car \\nmanufacturing business, it’s meant to bring dramatic improvements to the basic \\nperformance of Toyota cars. Increasing the efficiency of automobile design and development \\nthrough computational tests and simulations and constantly striving to improve are the keys \\nto making cars tha t have both excellent driving and environmental performance.  \\nToyota had handled high -performance computing workloads in an on -premises \\nenvironment. However, while continuing to use its existing on -premises resources, the \\ncompany began to look into cloud se rvices in order to flexibly meet requests from users, \\nsuch as quickly increasing resources and testing new technologies.', start_char_idx=2, end_char_idx=1632, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3840390421216894)], metadata={'8b887a7e-6bae-4d12-9677-a5c3b85527c5': {'page_label': '2', 'file_name': '2.Toyota.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\2.Toyota.PDF', 'file_type': 'application/pdf', 'file_size': 971839, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, 'df884c45-40af-4825-bf3f-362f196b1f53': {'page_label': '1', 'file_name': '2.Toyota.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\2.Toyota.PDF', 'file_type': 'application/pdf', 'file_size': 971839, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20a6616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"IFFCO is a leading agribusiness company in India, with a focus on the fertilizer industry. It was established in 1964 and has since grown to become one of the largest fertilizer companies in the country. IFFCO's product portfolio includes urea, DAP, NPK, and other fertilizers, as well as agrochemicals and seeds. The company has a strong presence in the domestic market and also exports its products to several countries around the world. In addition to its fertilizer business, IFFCO also provides financial services to farmers through its subsidiary, IFFCO Kisan Sanchar Limited.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", source_nodes=[NodeWithScore(node=TextNode(id_='a2aeeeb8-4248-4e57-954f-218d377ac0ec', embedding=None, metadata={'page_label': '5', 'file_name': '1.IFFCO Oracle Cloud.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\1.IFFCO Oracle Cloud.PDF', 'file_type': 'application/pdf', 'file_size': 1389131, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='10fbeb9c-cb05-4f96-81d2-64dabe25d2ce', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': '1.IFFCO Oracle Cloud.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\1.IFFCO Oracle Cloud.PDF', 'file_type': 'application/pdf', 'file_size': 1389131, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, hash='3784461b2a26230f341d990eaf85a7b09c19fd7820570c763a20dd7f2d84f7df')}, text='5 \\n \\n \\nFigure 3. IFFCO’s hub architecture in the Mumbai region  \\nOCI WAF provides security to traffic following over the internet. All incoming traffic is sent \\nto the public subnet and the  Check Point layer -7 firewall . IFFCO preferred to use separate \\nVCNs for better isolation, more control, and reduced impact radius if a security breach \\noccurs. EBS supports business processes and financial reporting. Exadata is used to host \\ndatabases for EBS and legacy apps.  \\nThe disaster recovery site in the Hyderabad region provides active -active disaster recovery. If \\na failure occurs in the Mumbai region, applications continue to run in the disaster recove ry \\nsite without disruption. This disaster recovery site is used for both legacy applications and E -\\nBusiness Suite and hosts nonproduction environments, developer and Test. The OCI Lift \\nservices team ensured that IFFCO had a smooth migration and deployment.  The team also \\nconducted multiple workshops to enable IFFCO successfully run workloads on OCI but also \\nassisted with planning and designing a flexible and scalable networking architecture based on \\nhub-and-spoke model for production, non -production, and dis aster recovery environments for \\nWindows -based legacy applications, Oracle EBS, and Exadata on OCI.  \\n \\nVideo Link  \\nhttps://youtu.be/WXC2Q7dfVs4', start_char_idx=2, end_char_idx=1331, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.48466959919946434), NodeWithScore(node=TextNode(id_='932f0ca1-93ed-41bc-94ff-e1bef4f109bd', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1fbc8342-b4c7-47c4-a261-087b13b7536d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}, hash='20036ea7ca5615058a24335942c51106a7e7d5cbbefb1d8bc0c0a8d89a926d7e')}, text='response=query_engine.query(\"what is IFFCO?\")\\nresponse\\n\\n\\n# In[ ]:', start_char_idx=3, end_char_idx=68, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4844444764005663)], metadata={'a2aeeeb8-4248-4e57-954f-218d377ac0ec': {'page_label': '5', 'file_name': '1.IFFCO Oracle Cloud.PDF', 'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\1.IFFCO Oracle Cloud.PDF', 'file_type': 'application/pdf', 'file_size': 1389131, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-12'}, '932f0ca1-93ed-41bc-94ff-e1bef4f109bd': {'file_path': 'C:\\\\Users\\\\Mugilan\\\\LLM\\\\LLama2RAG\\\\LLama2RAG.ipynb', 'file_name': 'LLama2RAG.ipynb', 'file_size': 169922, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-29'}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=query_engine.query(\"what is IFFCO?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4b2ed96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response\u001b[38;5;241m=\u001b[39m\u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is Oracle cloud ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:53\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     52\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 53\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m dispatch_event(QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result))\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:190\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    187\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    188\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m    189\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m--> 190\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_synthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\base.py:242\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[1;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    239\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE,\n\u001b[0;32m    240\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[0;32m    241\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m--> 242\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m    243\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[0;32m    244\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    245\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    246\u001b[0m         ],\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    248\u001b[0m     )\n\u001b[0;32m    250\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    251\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\compact_and_refine.py:43\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m     44\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[0;32m     45\u001b[0m     text_chunks\u001b[38;5;241m=\u001b[39mnew_texts,\n\u001b[0;32m     46\u001b[0m     prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m     48\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:185\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_give_response_single(\n\u001b[0;32m    186\u001b[0m             query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    187\u001b[0m         )\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[0;32m    190\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_response_single(\n\u001b[0;32m    191\u001b[0m             prev_response, query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    192\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:240\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[1;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m         structured_response \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m    239\u001b[0m             StructuredRefineResponse,\n\u001b[1;32m--> 240\u001b[0m             program(\n\u001b[0;32m    241\u001b[0m                 context_str\u001b[38;5;241m=\u001b[39mcur_text_chunk,\n\u001b[0;32m    242\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    243\u001b[0m             ),\n\u001b[0;32m    244\u001b[0m         )\n\u001b[0;32m    245\u001b[0m         query_satisfied \u001b[38;5;241m=\u001b[39m structured_response\u001b[38;5;241m.\u001b[39mquery_satisfied\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:84\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m     82\u001b[0m     answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m     87\u001b[0m     )\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[38;5;241m=\u001b[39manswer, query_satisfied\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\llms\\llm.py:442\u001b[0m, in \u001b[0;36mLLM.predict\u001b[1;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_prompt(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[1;32m--> 442\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    444\u001b[0m parsed_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_output(output)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:359\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    343\u001b[0m     LLMCompletionStartEvent(\n\u001b[0;32m    344\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39mmodel_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m )\n\u001b[0;32m    350\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m    351\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m    352\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    356\u001b[0m     },\n\u001b[0;32m    357\u001b[0m )\n\u001b[1;32m--> 359\u001b[0m f_return_val \u001b[38;5;241m=\u001b[39m f(_self, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompletionResponseGen:\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\llama_index\\llms\\huggingface\\base.py:358\u001b[0m, in \u001b[0;36mHuggingFaceLLM.complete\u001b[1;34m(self, prompt, formatted, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m    356\u001b[0m         inputs\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 358\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m    360\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_new_tokens,\n\u001b[0;32m    361\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping_criteria,\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_kwargs,\n\u001b[0;32m    363\u001b[0m )\n\u001b[0;32m    364\u001b[0m completion_tokens \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;241m0\u001b[39m][inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) :]\n\u001b[0;32m    365\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mdecode(completion_tokens, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\generation\\utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1755\u001b[0m     )\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   1759\u001b[0m         input_ids,\n\u001b[0;32m   1760\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1761\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[0;32m   1762\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1763\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   1764\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1765\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1766\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1767\u001b[0m     )\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1773\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\generation\\utils.py:2397\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   2394\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2396\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2398\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2399\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2400\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2401\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2402\u001b[0m )\n\u001b[0;32m   2404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2405\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1164\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1161\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1163\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:968\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    957\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    958\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    959\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    965\u001b[0m         cache_position,\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    978\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:713\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    710\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 713\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    722\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:616\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    613\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    615\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m--> 616\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[0;32m    619\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\accelerate\\hooks.py:161\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 161\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\accelerate\\hooks.py:347\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[1;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    340\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    341\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    342\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[0;32m    343\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[0;32m    344\u001b[0m         ):\n\u001b[0;32m    345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[1;32m--> 347\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[0;32m    357\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[0;32m    358\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mugilan\\LLM\\LLama2RAG\\ragenv\\lib\\site-packages\\accelerate\\utils\\modeling.py:400\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[1;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[0;32m    398\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 400\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response=query_engine.query(\"what is Oracle cloud ?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b507320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0873e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e66e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdc865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
